# -*- coding: utf-8 -*-
"""CYP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iWBtNDCtkyJ105eQxI9u01yR4AC56jvQ
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/yield_df.csv')

df.head()

df.drop('Unnamed: 0',axis=1,inplace=True)

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

def isStr(obj):
    try:
        float(obj)
        return False
    except:
        return True
to_drop = df[df['average_rain_fall_mm_per_year'].apply(isStr)].index

df = df.drop(to_drop)

df

df['average_rain_fall_mm_per_year'] = df['average_rain_fall_mm_per_year'].astype(np.float64)

len(df['Area'].unique())

plt.figure(figsize=(15,20))
sns.countplot(y=df['Area'])
plt.show()

(df['Area'].value_counts() < 500).sum()

country = df['Area'].unique()
yield_per_country = []
for state in country:
    yield_per_country.append(df[df['Area']==state]['hg/ha_yield'].sum())

df['hg/ha_yield'].sum()

yield_per_country

plt.figure(figsize=(15, 20))
sns.barplot(y=country, x=yield_per_country)

sns.countplot(y=df['Item'])

crops = df['Item'].unique()
yield_per_crop = []
for crop in crops:
    yield_per_crop.append(df[df['Item']==crop]['hg/ha_yield'].sum())

sns.barplot(y=crops,x=yield_per_crop)

col = ['Year', 'average_rain_fall_mm_per_year','pesticides_tonnes', 'avg_temp', 'Area', 'Item', 'hg/ha_yield']
df = df[col]
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

df.head(3)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0, shuffle=True)

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
ohe = OneHotEncoder(drop='first')
scale = StandardScaler()

preprocesser = ColumnTransformer(
        transformers = [
            ('StandardScale', scale, [0, 1, 2, 3]),
            ('OHE', ohe, [4, 5]),
        ],
        remainder='passthrough'
)

X_train_dummy = preprocesser.fit_transform(X_train)
X_test_dummy = preprocesser.transform(X_test)

preprocesser.get_feature_names_out(col[:-1])

from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, explained_variance_score
import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import make_scorer

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

rmse_scorer = make_scorer(rmse, greater_is_better=False)

# Define cross-validation strategy
cv = KFold(n_splits=5, shuffle=True, random_state=42)

models = {
    'Linear Regression': LinearRegression(),
    'Lasso': Lasso(),
    'Ridge': Ridge(),
    'Decision Tree': DecisionTreeRegressor()
}

for name, model in models.items():
    mae_scores = cross_val_score(model, X_train_dummy, y_train, scoring='neg_mean_absolute_error', cv=cv)
    r2_scores = cross_val_score(model, X_train_dummy, y_train, scoring='r2', cv=cv)
    rmse_scores = cross_val_score(model, X_train_dummy, y_train, scoring=rmse_scorer, cv=cv)

    print(f"{name}:")
    print(f"  Mean MAE  = {-np.mean(mae_scores):.2f}")
    print(f"  Mean RMSE = {-np.mean(rmse_scores):.2f}")
    print(f"  Mean R²   = {np.mean(r2_scores):.4f}\n")

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'max_depth': [3, 5, 10, 20, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8],
    'max_features': [None, 'auto', 'sqrt', 'log2']
}

# Initialize the model
dtr = DecisionTreeRegressor(random_state=42)

# GridSearchCV setup
grid_search = GridSearchCV(
    estimator=dtr,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

# Fit to training data
grid_search.fit(X_train_dummy, y_train)

# Best parameters and score
print("Best Parameters:\n", grid_search.best_params_)
print("\nBest MSE Score:", -grid_search.best_score_)

# Use the best model to predict
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_dummy)

# Evaluate final model
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
mae = mean_absolute_error(y_test, y_pred_best)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))
r2 = r2_score(y_test, y_pred_best)

print("\nFinal Evaluation on Test Set:")
print(f"MAE  = {mae:.2f}")
print(f"RMSE = {rmse:.2f}")
print(f"R²   = {r2:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd  # Import pandas


# Get feature names using get_feature_names_out()
feature_names = preprocesser.get_feature_names_out(col[:-1])

# Get importances from the best decision tree
importances = best_model.feature_importances_

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(data=feature_importance_df.head(15), x='Importance', y='Feature', palette='viridis')
plt.title('Top 15 Important Features for Crop Yield Prediction')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()